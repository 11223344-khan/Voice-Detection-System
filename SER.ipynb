{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFSdYuK80tpOTDswmkUV7u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b7724d8773104c01a9863bda88d89794": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Upload and Classify File",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_33059199d822456b9dca2e6b2a2a869f",
            "style": "IPY_MODEL_86d417f04e9b4285bd1e1d6c81fc3431",
            "tooltip": ""
          }
        },
        "33059199d822456b9dca2e6b2a2a869f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86d417f04e9b4285bd1e1d6c81fc3431": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/11223344-khan/11223344-khan/blob/main/SER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some Important Installations"
      ],
      "metadata": {
        "id": "C34u3BXVsKsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install resampy==0.2.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfFuAstAeOgN",
        "outputId": "71a2798f-7cac-4d96-8f75-84220091e798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting resampy==0.2.2\n",
            "  Downloading resampy-0.2.2.tar.gz (323 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/323.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m317.4/323.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.4/323.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.10/dist-packages (from resampy==0.2.2) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.13 in /usr/local/lib/python3.10/dist-packages (from resampy==0.2.2) (1.11.4)\n",
            "Requirement already satisfied: numba>=0.32 in /usr/local/lib/python3.10/dist-packages (from resampy==0.2.2) (0.58.1)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.10/dist-packages (from resampy==0.2.2) (1.16.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.32->resampy==0.2.2) (0.41.1)\n",
            "Building wheels for collected packages: resampy\n",
            "  Building wheel for resampy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for resampy: filename=resampy-0.2.2-py3-none-any.whl size=320706 sha256=1119b7918fe6f0609247dee62a0edd61a914c690ebd708a617cde87cf076617c\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/a0/79/29e61754e5b3941ad4c7d01bf5bea99768e64e4bdd3180f32b\n",
            "Successfully built resampy\n",
            "Installing collected packages: resampy\n",
            "Successfully installed resampy-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmEJtPOmX7Py",
        "outputId": "b6f5f1d1-cf47-424d-ea7c-71a4e3a49664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.5.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.7)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (4.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (23.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipywidgets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBWIbvma3C0_",
        "outputId": "9c639bff-5f85-4c07-f06a-9576b0a809a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.6.6)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.9)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.2)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.5.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.9.2)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.8)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.0)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.19.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.12)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.1.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.1.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.19.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.19.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.15.2)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.7.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.19.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Necessary Libraries"
      ],
      "metadata": {
        "id": "hhWLhIdSsoNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import joblib\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import resampy\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
        "import ipywidgets as widgets\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "xpRhxKWTYMFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount the Drive"
      ],
      "metadata": {
        "id": "9S4RbGST-kQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OE9vuGobYYn3",
        "outputId": "24fd5fe1-440b-4824-b935-3a399a92125f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Ravdess dataset and preprocess audio files"
      ],
      "metadata": {
        "id": "f8EecSQG-Q8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Ravdess dataset\n",
        "Ravdess = pd.DataFrame({\n",
        "    'emotions': [],\n",
        "    'path': []\n",
        "})\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "Ravdess_dir = \"/content/drive/MyDrive/Audio_Recognition/Ravedess\"\n",
        "for actor_folder in os.listdir(Ravdess_dir):\n",
        "    actor_folder_path = os.path.join(Ravdess_dir, actor_folder)\n",
        "    for file_name in os.listdir(actor_folder_path):\n",
        "        file_path = os.path.join(actor_folder_path, file_name)\n",
        "        emo_id = file_name.split('-')[2]\n",
        "        if emo_id == '01':\n",
        "            emo = 'neutral'\n",
        "        elif emo_id == '02':\n",
        "            emo = 'calm'\n",
        "        elif emo_id == '03':\n",
        "            emo = 'happy'\n",
        "        elif emo_id == '04':\n",
        "            emo = 'sad'\n",
        "        elif emo_id == '05':\n",
        "            emo = 'angry'\n",
        "        elif emo_id == '06':\n",
        "            emo = 'fearful'\n",
        "        elif emo_id == '07':\n",
        "            emo = 'disgust'\n",
        "        else:\n",
        "            emo = 'surprised'\n",
        "        new_data = pd.DataFrame({'emotions': [emo], 'path': [file_path]})\n",
        "        Ravdess = pd.concat([Ravdess, new_data], ignore_index=True)"
      ],
      "metadata": {
        "id": "MIn7Bt2UklYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ravdess"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "PHnUEKFvlGBs",
        "outputId": "6dfe71e4-9509-49b4-bd58-ed560b79dc9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       emotions                                               path\n",
              "0          calm  /content/drive/MyDrive/Audio_Recognition/Raved...\n",
              "1       neutral  /content/drive/MyDrive/Audio_Recognition/Raved...\n",
              "2          calm  /content/drive/MyDrive/Audio_Recognition/Raved...\n",
              "3          calm  /content/drive/MyDrive/Audio_Recognition/Raved...\n",
              "4          calm  /content/drive/MyDrive/Audio_Recognition/Raved...\n",
              "...         ...                                                ...\n",
              "1435        sad  /content/drive/MyDrive/Audio_Recognition/Raved...\n",
              "1436  surprised  /content/drive/MyDrive/Audio_Recognition/Raved...\n",
              "1437      angry  /content/drive/MyDrive/Audio_Recognition/Raved...\n",
              "1438      angry  /content/drive/MyDrive/Audio_Recognition/Raved...\n",
              "1439    fearful  /content/drive/MyDrive/Audio_Recognition/Raved...\n",
              "\n",
              "[1440 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-262a6435-dfab-48fc-989f-e0492d45fa99\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotions</th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>calm</td>\n",
              "      <td>/content/drive/MyDrive/Audio_Recognition/Raved...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neutral</td>\n",
              "      <td>/content/drive/MyDrive/Audio_Recognition/Raved...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>calm</td>\n",
              "      <td>/content/drive/MyDrive/Audio_Recognition/Raved...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>calm</td>\n",
              "      <td>/content/drive/MyDrive/Audio_Recognition/Raved...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>calm</td>\n",
              "      <td>/content/drive/MyDrive/Audio_Recognition/Raved...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1435</th>\n",
              "      <td>sad</td>\n",
              "      <td>/content/drive/MyDrive/Audio_Recognition/Raved...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1436</th>\n",
              "      <td>surprised</td>\n",
              "      <td>/content/drive/MyDrive/Audio_Recognition/Raved...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1437</th>\n",
              "      <td>angry</td>\n",
              "      <td>/content/drive/MyDrive/Audio_Recognition/Raved...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1438</th>\n",
              "      <td>angry</td>\n",
              "      <td>/content/drive/MyDrive/Audio_Recognition/Raved...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1439</th>\n",
              "      <td>fearful</td>\n",
              "      <td>/content/drive/MyDrive/Audio_Recognition/Raved...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1440 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-262a6435-dfab-48fc-989f-e0492d45fa99')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-262a6435-dfab-48fc-989f-e0492d45fa99 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-262a6435-dfab-48fc-989f-e0492d45fa99');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fe848b27-88e9-451d-bd6e-ea5882bd9762\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fe848b27-88e9-451d-bd6e-ea5882bd9762')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fe848b27-88e9-451d-bd6e-ea5882bd9762 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_48c0960e-5c7a-407f-80dd-e062e840ab22\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('Ravdess')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_48c0960e-5c7a-407f-80dd-e062e840ab22 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('Ravdess');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display Emotion Distribution in Ravdess Dataset"
      ],
      "metadata": {
        "id": "wMUiBptp-cdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ravdess['emotions'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqDO-snWlTyk",
        "outputId": "9bbe0850-00a3-4313-ebdd-0e949ee87930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "calm         192\n",
              "happy        192\n",
              "disgust      192\n",
              "fearful      192\n",
              "sad          192\n",
              "surprised    192\n",
              "angry        192\n",
              "neutral       96\n",
              "Name: emotions, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define and List Audio Files in the Directory"
      ],
      "metadata": {
        "id": "zJG6QgRL-09G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to your audio files directory\n",
        "audio_files_dir = '/content/drive/MyDrive/Audio_Recognition/Ravedess'\n",
        "\n",
        "# List audio files in the directory\n",
        "audio_files = [os.path.join(audio_files_dir, file) for file in os.listdir(audio_files_dir)]"
      ],
      "metadata": {
        "id": "hers3JA4YcCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List the contents of the dataset folder.\n",
        "os.listdir(audio_files_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64mMiLBDYkK7",
        "outputId": "fb08704a-3d2c-4461-edce-66b2e1996293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Actor_20',\n",
              " 'Actor_17',\n",
              " 'Actor_23',\n",
              " 'Actor_15',\n",
              " 'Actor_22',\n",
              " 'Actor_18',\n",
              " 'Actor_21',\n",
              " 'Actor_19',\n",
              " 'Actor_16',\n",
              " 'Actor_24',\n",
              " 'Actor_09',\n",
              " 'Actor_10',\n",
              " 'Actor_05',\n",
              " 'Actor_08',\n",
              " 'Actor_12',\n",
              " 'Actor_06',\n",
              " 'Actor_11',\n",
              " 'Actor_07',\n",
              " 'Actor_14',\n",
              " 'Actor_13',\n",
              " 'Actor_01',\n",
              " 'Actor_04',\n",
              " 'Actor_02',\n",
              " 'Actor_03']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to Extract Audio Features\n",
        "Function extract_features that takes a file path as input and extracts audio features, including (MFCC), chroma feature, and mel spectrogram features. The function uses the librosa library to load the audio file, compute the necessary features, and returns a concatenated array of these features."
      ],
      "metadata": {
        "id": "X5T4dF_S_Iue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract features from audio files\n",
        "def extract_features(file_path, mfcc=True, chroma=True, mel=True):\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')\n",
        "        if chroma:\n",
        "            stft = np.abs(librosa.stft(audio))\n",
        "        result = np.array([])\n",
        "        if mfcc:\n",
        "            mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
        "            result = np.hstack((result, mfccs))\n",
        "        if chroma:\n",
        "            chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
        "            result = np.hstack((result, chroma))\n",
        "        if mel:\n",
        "            mel = np.mean(librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_fft=2048, hop_length=512).T, axis=0)\n",
        "            result = np.hstack((result, mel))\n",
        "    return result"
      ],
      "metadata": {
        "id": "5ye8rwMmYm-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to Preprocess Audio Data and Extract Features\n",
        "This code defines a function preprocess_data that takes the directory path containing audio files, iterates through each file, extracts emotion labels and audio features using the extract_features function, and returns two lists."
      ],
      "metadata": {
        "id": "qGhOn_v__phI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess and extract features from a list of audio files\n",
        "def preprocess_data(audio_files_dir):\n",
        "    emotions = []\n",
        "    features = []\n",
        "    for actor_folder in os.listdir(audio_files_dir):\n",
        "        actor_folder_path = os.path.join(audio_files_dir, actor_folder)\n",
        "        for file_name in os.listdir(actor_folder_path):\n",
        "            file_path = os.path.join(actor_folder_path, file_name)\n",
        "            emotion = file_name.split('-')[2]\n",
        "            if emotion not in emotions:\n",
        "                emotions.append(emotion)\n",
        "            feature = extract_features(file_path)\n",
        "            features.append([feature, emotion])\n",
        "    return emotions, features"
      ],
      "metadata": {
        "id": "zhphC_nBYuf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Preprocess Audio Data\n",
        "This code calls the preprocess_data function to load audio data from the specified directory (audio_files_dir). It extracts emotion labels and audio features for each file, storing the unique emotion labels in the emotions list and the corresponding feature-emotion pairs in the features list."
      ],
      "metadata": {
        "id": "HtecZFENACN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "emotions, features = preprocess_data(audio_files_dir)"
      ],
      "metadata": {
        "id": "bvAoCgHHYzws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting the Dataset"
      ],
      "metadata": {
        "id": "TQtG70JaANaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X = np.array([x[0] for x in features])\n",
        "y = np.array([y[1] for y in features])"
      ],
      "metadata": {
        "id": "R3g9zuh2Y4HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sGWoZi3ezk7",
        "outputId": "6be8a4bf-3e05-41bd-99e9-06d977809dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-8.52922668e+02,  3.84620819e+01, -4.90387869e+00, ...,\n",
              "         8.31060873e-08,  1.16610686e-08,  1.39717815e-09],\n",
              "       [-7.13109070e+02,  3.78296471e+01, -1.04684496e+01, ...,\n",
              "         7.25710265e-07,  1.32387200e-07,  1.47552113e-08],\n",
              "       [-7.95232117e+02,  4.12144318e+01, -1.22606478e+01, ...,\n",
              "         5.21315862e-08,  1.36715341e-08,  1.11911969e-09],\n",
              "       ...,\n",
              "       [-3.25668671e+02,  4.84727020e+01, -1.98268089e+01, ...,\n",
              "         1.35883602e-04,  2.19930935e-05,  2.14702618e-06],\n",
              "       [-3.31223969e+02,  4.62784882e+01, -1.89334869e+01, ...,\n",
              "         1.32261193e-04,  1.91710824e-05,  1.73276806e-06],\n",
              "       [-5.81411987e+02,  6.99668579e+01, -7.83518362e+00, ...,\n",
              "         6.76299851e-06,  1.65709207e-06,  1.77608470e-07]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_LvNMYAe2Sr",
        "outputId": "d86346ee-a2da-465c-ac0e-d5866201e21c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Label Encoding and One-Hot Encoding for Emotion Labels\n",
        "LabelEncoder (le) is used to transform the categorical emotion labels (y) into numerical representations. The fit_transform method of the LabelEncoder is applied to the emotion labels, and then to_categorical is used to perform one-hot encoding on the transformed labels, converting them into binary vectors.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X9qut-1SATFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Label encoding and one-hot encoding\n",
        "le = LabelEncoder()\n",
        "y = to_categorical(le.fit_transform(y))"
      ],
      "metadata": {
        "id": "WpfV-vY8bszH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "5ogZNEzlbxlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reshaping the Data"
      ],
      "metadata": {
        "id": "vB7L36C_AwUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the data for CNN input\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)"
      ],
      "metadata": {
        "id": "H6c3fz9Mb1sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalize Input Data using StandardScaler\n",
        "This code utilizes the StandardScaler (scaler) to normalize the input data. The fit_transform method is applied to the training data (X_train) to calculate the mean and standard deviation and perform the normalization. The same transformation is then applied to the test data (X_test)."
      ],
      "metadata": {
        "id": "S988C1CCBGHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the input data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
        "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)"
      ],
      "metadata": {
        "id": "ftApABmYb5or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the input data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
        "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)"
      ],
      "metadata": {
        "id": "y0arqoKNA96b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define and Train Convolutional Neural Network (CNN) Model\n",
        "This code defines a Convolutional Neural Network (CNN) model using the Keras Sequential API. The model consists of convolutional layers with max pooling, dropout layers for regularization, and dense layers. The model is compiled with the categorical crossentropy loss function and the Adam optimizer. During training, the ModelCheckpoint callback is used to save the best model based on validation accuracy. The model is trained on the training data (X_train, y_train) for 100 epochs with a batch size of 32."
      ],
      "metadata": {
        "id": "WglOKW8fBaK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv1D(128, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(emotions), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Save the best model during training\n",
        "checkpoint = ModelCheckpoint(\"best_model.h5\", monitor='val_accuracy', save_best_only=True, mode='max')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, verbose=1, callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqCtKe3Gb-DE",
        "outputId": "b1663e73-cb0d-4682-becf-cae79dd69681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "36/36 [==============================] - 4s 74ms/step - loss: 2.0276 - accuracy: 0.1832 - val_loss: 1.9668 - val_accuracy: 0.2188\n",
            "Epoch 2/100\n",
            " 1/36 [..............................] - ETA: 2s - loss: 2.0014 - accuracy: 0.1875"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36/36 [==============================] - 3s 75ms/step - loss: 1.9087 - accuracy: 0.2439 - val_loss: 1.8498 - val_accuracy: 0.3368\n",
            "Epoch 3/100\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 1.8431 - accuracy: 0.2830 - val_loss: 1.8127 - val_accuracy: 0.3368\n",
            "Epoch 4/100\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 1.7810 - accuracy: 0.3021 - val_loss: 1.7539 - val_accuracy: 0.3715\n",
            "Epoch 5/100\n",
            "36/36 [==============================] - 2s 46ms/step - loss: 1.7622 - accuracy: 0.3064 - val_loss: 1.7092 - val_accuracy: 0.3646\n",
            "Epoch 6/100\n",
            "36/36 [==============================] - 2s 44ms/step - loss: 1.6898 - accuracy: 0.3585 - val_loss: 1.7330 - val_accuracy: 0.3056\n",
            "Epoch 7/100\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.6628 - accuracy: 0.3698 - val_loss: 1.7073 - val_accuracy: 0.3472\n",
            "Epoch 8/100\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.6450 - accuracy: 0.3663 - val_loss: 1.7032 - val_accuracy: 0.3507\n",
            "Epoch 9/100\n",
            "36/36 [==============================] - 2s 47ms/step - loss: 1.6416 - accuracy: 0.3750 - val_loss: 1.6923 - val_accuracy: 0.3924\n",
            "Epoch 10/100\n",
            "36/36 [==============================] - 3s 73ms/step - loss: 1.5897 - accuracy: 0.4036 - val_loss: 1.5856 - val_accuracy: 0.4479\n",
            "Epoch 11/100\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 1.5632 - accuracy: 0.3906 - val_loss: 1.5876 - val_accuracy: 0.4167\n",
            "Epoch 12/100\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 1.5343 - accuracy: 0.4306 - val_loss: 1.5657 - val_accuracy: 0.4340\n",
            "Epoch 13/100\n",
            "36/36 [==============================] - 2s 44ms/step - loss: 1.4990 - accuracy: 0.4332 - val_loss: 1.5543 - val_accuracy: 0.4618\n",
            "Epoch 14/100\n",
            "36/36 [==============================] - 2s 44ms/step - loss: 1.4598 - accuracy: 0.4696 - val_loss: 1.5584 - val_accuracy: 0.4514\n",
            "Epoch 15/100\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4536 - accuracy: 0.4661 - val_loss: 1.5399 - val_accuracy: 0.4444\n",
            "Epoch 16/100\n",
            "36/36 [==============================] - 2s 44ms/step - loss: 1.4267 - accuracy: 0.4635 - val_loss: 1.4867 - val_accuracy: 0.4479\n",
            "Epoch 17/100\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.4026 - accuracy: 0.4818 - val_loss: 1.5545 - val_accuracy: 0.4549\n",
            "Epoch 18/100\n",
            "36/36 [==============================] - 2s 48ms/step - loss: 1.3810 - accuracy: 0.4887 - val_loss: 1.4809 - val_accuracy: 0.4549\n",
            "Epoch 19/100\n",
            "36/36 [==============================] - 3s 73ms/step - loss: 1.3596 - accuracy: 0.4748 - val_loss: 1.5402 - val_accuracy: 0.4306\n",
            "Epoch 20/100\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 1.3558 - accuracy: 0.4913 - val_loss: 1.4443 - val_accuracy: 0.4618\n",
            "Epoch 21/100\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 1.3629 - accuracy: 0.4861 - val_loss: 1.5164 - val_accuracy: 0.4306\n",
            "Epoch 22/100\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.3169 - accuracy: 0.5130 - val_loss: 1.4349 - val_accuracy: 0.4549\n",
            "Epoch 23/100\n",
            "36/36 [==============================] - 2s 46ms/step - loss: 1.2938 - accuracy: 0.5104 - val_loss: 1.4670 - val_accuracy: 0.4861\n",
            "Epoch 24/100\n",
            "36/36 [==============================] - 2s 44ms/step - loss: 1.2488 - accuracy: 0.5234 - val_loss: 1.4325 - val_accuracy: 0.4688\n",
            "Epoch 25/100\n",
            "36/36 [==============================] - 2s 45ms/step - loss: 1.2605 - accuracy: 0.5182 - val_loss: 1.4400 - val_accuracy: 0.4792\n",
            "Epoch 26/100\n",
            "36/36 [==============================] - 2s 47ms/step - loss: 1.2326 - accuracy: 0.5477 - val_loss: 1.4181 - val_accuracy: 0.4931\n",
            "Epoch 27/100\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 1.2155 - accuracy: 0.5382 - val_loss: 1.3729 - val_accuracy: 0.4931\n",
            "Epoch 28/100\n",
            "36/36 [==============================] - 3s 75ms/step - loss: 1.2137 - accuracy: 0.5347 - val_loss: 1.4083 - val_accuracy: 0.5208\n",
            "Epoch 29/100\n",
            "36/36 [==============================] - 2s 67ms/step - loss: 1.1814 - accuracy: 0.5564 - val_loss: 1.3995 - val_accuracy: 0.5069\n",
            "Epoch 30/100\n",
            "36/36 [==============================] - 2s 44ms/step - loss: 1.1561 - accuracy: 0.5807 - val_loss: 1.4492 - val_accuracy: 0.5104\n",
            "Epoch 31/100\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.1256 - accuracy: 0.5807 - val_loss: 1.4528 - val_accuracy: 0.5069\n",
            "Epoch 32/100\n",
            "36/36 [==============================] - 2s 47ms/step - loss: 1.1572 - accuracy: 0.5425 - val_loss: 1.3849 - val_accuracy: 0.5417\n",
            "Epoch 33/100\n",
            "36/36 [==============================] - 2s 45ms/step - loss: 1.0901 - accuracy: 0.5903 - val_loss: 1.3186 - val_accuracy: 0.5382\n",
            "Epoch 34/100\n",
            "36/36 [==============================] - 2s 44ms/step - loss: 1.0935 - accuracy: 0.5833 - val_loss: 1.3149 - val_accuracy: 0.5312\n",
            "Epoch 35/100\n",
            "36/36 [==============================] - 2s 47ms/step - loss: 1.0734 - accuracy: 0.6024 - val_loss: 1.3122 - val_accuracy: 0.5451\n",
            "Epoch 36/100\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 1.0421 - accuracy: 0.6120 - val_loss: 1.2678 - val_accuracy: 0.5590\n",
            "Epoch 37/100\n",
            "36/36 [==============================] - 3s 73ms/step - loss: 1.0676 - accuracy: 0.6137 - val_loss: 1.2851 - val_accuracy: 0.5625\n",
            "Epoch 38/100\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 1.0374 - accuracy: 0.6111 - val_loss: 1.2875 - val_accuracy: 0.5521\n",
            "Epoch 39/100\n",
            "36/36 [==============================] - 2s 45ms/step - loss: 0.9979 - accuracy: 0.6389 - val_loss: 1.3008 - val_accuracy: 0.5139\n",
            "Epoch 40/100\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 1.0144 - accuracy: 0.6189 - val_loss: 1.2673 - val_accuracy: 0.5625\n",
            "Epoch 41/100\n",
            "36/36 [==============================] - 2s 45ms/step - loss: 0.9694 - accuracy: 0.6319 - val_loss: 1.3947 - val_accuracy: 0.5556\n",
            "Epoch 42/100\n",
            "36/36 [==============================] - 2s 47ms/step - loss: 1.0121 - accuracy: 0.6224 - val_loss: 1.3692 - val_accuracy: 0.5764\n",
            "Epoch 43/100\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9470 - accuracy: 0.6606 - val_loss: 1.3158 - val_accuracy: 0.5764\n",
            "Epoch 44/100\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.9269 - accuracy: 0.6762 - val_loss: 1.2851 - val_accuracy: 0.6042\n",
            "Epoch 45/100\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.9521 - accuracy: 0.6606 - val_loss: 1.2290 - val_accuracy: 0.5729\n",
            "Epoch 46/100\n",
            "36/36 [==============================] - 3s 76ms/step - loss: 0.9565 - accuracy: 0.6632 - val_loss: 1.2398 - val_accuracy: 0.6215\n",
            "Epoch 47/100\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.9089 - accuracy: 0.6840 - val_loss: 1.3409 - val_accuracy: 0.5938\n",
            "Epoch 48/100\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8895 - accuracy: 0.6727 - val_loss: 1.2937 - val_accuracy: 0.6111\n",
            "Epoch 49/100\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8796 - accuracy: 0.6797 - val_loss: 1.3404 - val_accuracy: 0.5833\n",
            "Epoch 50/100\n",
            "36/36 [==============================] - 2s 44ms/step - loss: 0.8840 - accuracy: 0.6788 - val_loss: 1.2734 - val_accuracy: 0.6181\n",
            "Epoch 51/100\n",
            "36/36 [==============================] - 2s 44ms/step - loss: 0.8226 - accuracy: 0.6884 - val_loss: 1.2635 - val_accuracy: 0.6389\n",
            "Epoch 52/100\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8065 - accuracy: 0.7179 - val_loss: 1.3530 - val_accuracy: 0.6181\n",
            "Epoch 53/100\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8361 - accuracy: 0.6866 - val_loss: 1.2841 - val_accuracy: 0.5903\n",
            "Epoch 54/100\n",
            "36/36 [==============================] - 3s 73ms/step - loss: 0.8263 - accuracy: 0.6953 - val_loss: 1.2979 - val_accuracy: 0.6250\n",
            "Epoch 55/100\n",
            "36/36 [==============================] - 3s 74ms/step - loss: 0.7788 - accuracy: 0.7144 - val_loss: 1.2307 - val_accuracy: 0.6424\n",
            "Epoch 56/100\n",
            "36/36 [==============================] - 2s 48ms/step - loss: 0.8009 - accuracy: 0.7049 - val_loss: 1.2119 - val_accuracy: 0.6250\n",
            "Epoch 57/100\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.7746 - accuracy: 0.7101 - val_loss: 1.2351 - val_accuracy: 0.6354\n",
            "Epoch 58/100\n",
            "36/36 [==============================] - 2s 45ms/step - loss: 0.7690 - accuracy: 0.7127 - val_loss: 1.3078 - val_accuracy: 0.6458\n",
            "Epoch 59/100\n",
            "36/36 [==============================] - 2s 45ms/step - loss: 0.7399 - accuracy: 0.7344 - val_loss: 1.2779 - val_accuracy: 0.6424\n",
            "Epoch 60/100\n",
            "36/36 [==============================] - 2s 44ms/step - loss: 0.7216 - accuracy: 0.7361 - val_loss: 1.2647 - val_accuracy: 0.6389\n",
            "Epoch 61/100\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.7607 - accuracy: 0.7222 - val_loss: 1.3964 - val_accuracy: 0.6181\n",
            "Epoch 62/100\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.7261 - accuracy: 0.7422 - val_loss: 1.2138 - val_accuracy: 0.6389\n",
            "Epoch 63/100\n",
            "36/36 [==============================] - 3s 73ms/step - loss: 0.7063 - accuracy: 0.7387 - val_loss: 1.2342 - val_accuracy: 0.6111\n",
            "Epoch 64/100\n",
            "36/36 [==============================] - 2s 69ms/step - loss: 0.7057 - accuracy: 0.7318 - val_loss: 1.2174 - val_accuracy: 0.6389\n",
            "Epoch 65/100\n",
            "36/36 [==============================] - 2s 44ms/step - loss: 0.6374 - accuracy: 0.7760 - val_loss: 1.2544 - val_accuracy: 0.6250\n",
            "Epoch 66/100\n",
            "36/36 [==============================] - 2s 45ms/step - loss: 0.6632 - accuracy: 0.7526 - val_loss: 1.3070 - val_accuracy: 0.6424\n",
            "Epoch 67/100\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.6640 - accuracy: 0.7543 - val_loss: 1.4442 - val_accuracy: 0.6285\n",
            "Epoch 68/100\n",
            "36/36 [==============================] - 2s 45ms/step - loss: 0.6706 - accuracy: 0.7726 - val_loss: 1.2929 - val_accuracy: 0.6424\n",
            "Epoch 69/100\n",
            "36/36 [==============================] - 2s 47ms/step - loss: 0.6888 - accuracy: 0.7517 - val_loss: 1.3592 - val_accuracy: 0.6597\n",
            "Epoch 70/100\n",
            "36/36 [==============================] - 2s 45ms/step - loss: 0.6364 - accuracy: 0.7517 - val_loss: 1.1007 - val_accuracy: 0.6632\n",
            "Epoch 71/100\n",
            "36/36 [==============================] - 2s 67ms/step - loss: 0.6464 - accuracy: 0.7760 - val_loss: 1.3335 - val_accuracy: 0.6840\n",
            "Epoch 72/100\n",
            "36/36 [==============================] - 2s 69ms/step - loss: 0.6416 - accuracy: 0.7613 - val_loss: 1.3312 - val_accuracy: 0.6389\n",
            "Epoch 73/100\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.6119 - accuracy: 0.7847 - val_loss: 1.3444 - val_accuracy: 0.6771\n",
            "Epoch 74/100\n",
            "36/36 [==============================] - 2s 45ms/step - loss: 0.5766 - accuracy: 0.7951 - val_loss: 1.3401 - val_accuracy: 0.6632\n",
            "Epoch 75/100\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.5850 - accuracy: 0.8003 - val_loss: 1.3122 - val_accuracy: 0.6597\n",
            "Epoch 76/100\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.6122 - accuracy: 0.7769 - val_loss: 1.3389 - val_accuracy: 0.6701\n",
            "Epoch 77/100\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.5669 - accuracy: 0.7925 - val_loss: 1.2942 - val_accuracy: 0.6632\n",
            "Epoch 78/100\n",
            "36/36 [==============================] - 2s 45ms/step - loss: 0.5936 - accuracy: 0.7812 - val_loss: 1.2604 - val_accuracy: 0.6806\n",
            "Epoch 79/100\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.5348 - accuracy: 0.7986 - val_loss: 1.3343 - val_accuracy: 0.6840\n",
            "Epoch 80/100\n",
            "36/36 [==============================] - 3s 73ms/step - loss: 0.5391 - accuracy: 0.7951 - val_loss: 1.3186 - val_accuracy: 0.6910\n",
            "Epoch 81/100\n",
            "36/36 [==============================] - 3s 73ms/step - loss: 0.5545 - accuracy: 0.8186 - val_loss: 1.1208 - val_accuracy: 0.7153\n",
            "Epoch 82/100\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.4885 - accuracy: 0.8247 - val_loss: 1.1577 - val_accuracy: 0.6944\n",
            "Epoch 83/100\n",
            "36/36 [==============================] - 2s 45ms/step - loss: 0.5444 - accuracy: 0.8134 - val_loss: 1.3030 - val_accuracy: 0.6910\n",
            "Epoch 84/100\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.4804 - accuracy: 0.8368 - val_loss: 1.2903 - val_accuracy: 0.6701\n",
            "Epoch 85/100\n",
            "36/36 [==============================] - 2s 45ms/step - loss: 0.5010 - accuracy: 0.8212 - val_loss: 1.2837 - val_accuracy: 0.6632\n",
            "Epoch 86/100\n",
            "36/36 [==============================] - 2s 45ms/step - loss: 0.5231 - accuracy: 0.8194 - val_loss: 1.2159 - val_accuracy: 0.6875\n",
            "Epoch 87/100\n",
            "36/36 [==============================] - 2s 46ms/step - loss: 0.5406 - accuracy: 0.8125 - val_loss: 1.3657 - val_accuracy: 0.6562\n",
            "Epoch 88/100\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.4867 - accuracy: 0.8142 - val_loss: 1.3272 - val_accuracy: 0.6701\n",
            "Epoch 89/100\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.4957 - accuracy: 0.8108 - val_loss: 1.3756 - val_accuracy: 0.6840\n",
            "Epoch 90/100\n",
            "36/36 [==============================] - 3s 74ms/step - loss: 0.5561 - accuracy: 0.8134 - val_loss: 1.2770 - val_accuracy: 0.7049\n",
            "Epoch 91/100\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 0.5104 - accuracy: 0.8160 - val_loss: 1.2616 - val_accuracy: 0.6771\n",
            "Epoch 92/100\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.4663 - accuracy: 0.8359 - val_loss: 1.2158 - val_accuracy: 0.6979\n",
            "Epoch 93/100\n",
            "36/36 [==============================] - 2s 46ms/step - loss: 0.4791 - accuracy: 0.8212 - val_loss: 1.3165 - val_accuracy: 0.6736\n",
            "Epoch 94/100\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.4549 - accuracy: 0.8481 - val_loss: 1.2744 - val_accuracy: 0.6632\n",
            "Epoch 95/100\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.4813 - accuracy: 0.8229 - val_loss: 1.1976 - val_accuracy: 0.7049\n",
            "Epoch 96/100\n",
            "36/36 [==============================] - 2s 45ms/step - loss: 0.4387 - accuracy: 0.8490 - val_loss: 1.2698 - val_accuracy: 0.6979\n",
            "Epoch 97/100\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4661 - accuracy: 0.8455 - val_loss: 1.3160 - val_accuracy: 0.6910\n",
            "Epoch 98/100\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.4374 - accuracy: 0.8351 - val_loss: 1.2006 - val_accuracy: 0.7118\n",
            "Epoch 99/100\n",
            "36/36 [==============================] - 3s 73ms/step - loss: 0.4355 - accuracy: 0.8516 - val_loss: 1.2378 - val_accuracy: 0.6771\n",
            "Epoch 100/100\n",
            "36/36 [==============================] - 2s 46ms/step - loss: 0.4338 - accuracy: 0.8368 - val_loss: 1.2808 - val_accuracy: 0.6840\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7dbed9670850>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the best model"
      ],
      "metadata": {
        "id": "7miRlbRaBmex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model for evaluation\n",
        "model.load_weights(\"best_model.h5\")"
      ],
      "metadata": {
        "id": "FY6droFmcIxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "GeK_Puo3BsDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "_, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMmXRho3chAC",
        "outputId": "208bdf71-ba3c-48be-8588-71b0514d115d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 0s 11ms/step - loss: 1.1208 - accuracy: 0.7153\n",
            "Test Accuracy: 71.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the Model"
      ],
      "metadata": {
        "id": "Xqw4ft_tBzhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to a file\n",
        "joblib.dump(model, 'saved_model.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7XBYgIoch__",
        "outputId": "236f3378-2fd4-4073-c185-734e3f6f8321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['saved_model.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Model"
      ],
      "metadata": {
        "id": "FyWKw4AqB5Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model from the file\n",
        "loaded_model = joblib.load('saved_model.joblib')"
      ],
      "metadata": {
        "id": "B4OZf4lsgaaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "model = load_model(\"best_model.h5\")\n",
        "\n",
        "# Load the scaler used for normalization during training\n",
        "scaler = StandardScaler()"
      ],
      "metadata": {
        "id": "g3b9YawHgdRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Features from User Input Audio\n",
        "Defines a function extract_user_input_features to extract audio features, including MFCC, chroma, and mel spectrogram features, from a user's input audio file using the librosa library. The function is designed for efficient feature extraction from user-provided audio for analysis or model predictions."
      ],
      "metadata": {
        "id": "uva9S9ErCqg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract features from user input audio file\n",
        "def extract_user_input_features(file_path, mfcc=True, chroma=True, mel=True):\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')\n",
        "        if chroma:\n",
        "            stft = np.abs(librosa.stft(audio))\n",
        "        result = np.array([])\n",
        "        if mfcc:\n",
        "            mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
        "            result = np.hstack((result, mfccs))\n",
        "        if chroma:\n",
        "            chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
        "            result = np.hstack((result, chroma))\n",
        "        if mel:\n",
        "            mel = np.mean(librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_fft=2048, hop_length=512).T, axis=0)\n",
        "            result = np.hstack((result, mel))\n",
        "    return result"
      ],
      "metadata": {
        "id": "vm0JtbVYiZ8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict Emotion from User Input Audio\n",
        "This function predicts the emotion label from a user's input audio file. It extracts features, reshapes, normalizes using a specified scaler, and utilizes a pre-trained CNN model to make predictions. The result is the predicted emotion label based on the highest model probability."
      ],
      "metadata": {
        "id": "pGkcRqzvCy0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict emotion from user input\n",
        "def predict_emotion(file_path, scaler):\n",
        "    # Extract features from user input\n",
        "    user_input_features = extract_user_input_features(file_path)\n",
        "\n",
        "    # Reshape and normalize the input data\n",
        "    user_input_features = user_input_features.reshape(1, user_input_features.shape[0], 1)\n",
        "    user_input_features = scaler.transform(user_input_features.reshape(-1, user_input_features.shape[-1])).reshape(user_input_features.shape)\n",
        "\n",
        "    # Make a prediction\n",
        "    prediction = model.predict(user_input_features)\n",
        "\n",
        "    # Get the emotion label\n",
        "    emotion_label = np.argmax(prediction)\n",
        "\n",
        "    return emotion_label"
      ],
      "metadata": {
        "id": "tOufTtWPif85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Emotion Label to Text\n",
        "The function emotion_to_text that takes an emotion label and a list of emotions, and returns the corresponding text representation of the emotion. If the provided emotion label is out of bounds, it returns \"Unknown Emotion.\" This function is useful for converting numerical emotion labels to human-readable text."
      ],
      "metadata": {
        "id": "Bgt06FazDR3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert emotion label to text\n",
        "def emotion_to_text(emotion_label, emotions):\n",
        "    try:\n",
        "        emotion_text = emotions[emotion_label]\n",
        "    except IndexError:\n",
        "        emotion_text = \"Unknown Emotion\"\n",
        "    return emotion_text"
      ],
      "metadata": {
        "id": "UUGzrFucg9i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit the Scalar"
      ],
      "metadata": {
        "id": "gKan8UcZDdft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit and save the scaler\n",
        "scaler.fit(X_train.reshape(-1, X_train.shape[-1]))\n",
        "\n",
        "# Save the scaler to a file\n",
        "scaler_filename = \"scaler.save\"\n",
        "joblib.dump(scaler, scaler_filename)\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"best_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFQSJCyQkvmp",
        "outputId": "22f75a26-fb7d-49f5-cc26-2cfa430491d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continuous Emotion Prediction from User Input\n",
        "This code snippet creates an infinite loop prompting the user to enter the path to an audio file. It predicts the emotion from the user's input using the pre-trained model, converts the numerical prediction to text, and displays the result. The loop continues until the user enters 'exit' to quit. Any errors during the process are also handled and displayed."
      ],
      "metadata": {
        "id": "z_ngAFbtDrbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Infinite loop to keep asking the user for input\n",
        "while True:\n",
        "    # Get the user's voice input file path\n",
        "    user_input_file_path = input(\"Enter the path to the audio file ('exit' to quit): \")\n",
        "\n",
        "    # Check if the user wants to exit\n",
        "    if user_input_file_path.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        # Predict emotion and convert to text\n",
        "        # Assuming you have the list of emotions as specified in your dataset\n",
        "        dataset_emotions = [\"angry\", \"calm\", \"disgust\", \"fearful\", \"happy\", \"sad\", \"surprised\", \"neutral\"]\n",
        "\n",
        "        # Predict emotion and convert to text\n",
        "        predicted_emotion_label = predict_emotion(user_input_file_path, scaler)\n",
        "        predicted_emotion_text = emotion_to_text(predicted_emotion_label, dataset_emotions)\n",
        "\n",
        "        # Display the result\n",
        "        print(f\"Predicted Emotion: {predicted_emotion_text}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nazJmTIjl38",
        "outputId": "f949fa27-57f3-4058-fcdf-0d06cbe8d298"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the path to the audio file ('exit' to quit): /content/drive/MyDrive/Audio_Recognition/Ravedess/Actor_10/03-01-04-01-01-01-10.wav\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Predicted Emotion: fearful\n",
            "Enter the path to the audio file ('exit' to quit): /content/drive/MyDrive/Audio_Recognition/Ravedess/Actor_17/03-01-01-01-01-02-17.wav\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Predicted Emotion: calm\n",
            "Enter the path to the audio file ('exit' to quit): /content/drive/MyDrive/Audio_Recognition/Ravedess/Actor_21/03-01-01-01-01-01-21.wav\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Predicted Emotion: calm\n",
            "Enter the path to the audio file ('exit' to quit): quit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-8e798c0804e0>:4: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: [Errno 2] No such file or directory: 'quit'\n",
            "Enter the path to the audio file ('exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload and Classify Audio File with Pre-trained Model\n",
        "This code snippet loads a pre-trained neural network model (best_model.h5) and the associated scaler used during training. It defines a function triggered by a button click to upload an audio file, predict its emotion using the loaded model, and display the result. The button is created using the ipywidgets library, allowing users to interactively upload and classify audio files."
      ],
      "metadata": {
        "id": "jzDySudHEBZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "model = load_model(\"best_model.h5\")\n",
        "\n",
        "# Load the scaler used for normalization during training\n",
        "scaler_filename = \"scaler.save\"\n",
        "scaler = joblib.load(scaler_filename)\n",
        "\n",
        "# Define the function that will be called when the button is clicked\n",
        "def on_button_click(b):\n",
        "    uploaded = files.upload()\n",
        "    file_name = list(uploaded.keys())[0]\n",
        "    tmp_path = f'/tmp/{file_name}'\n",
        "    os.rename(file_name, tmp_path)\n",
        "    global uploaded_file_path\n",
        "    uploaded_file_path = tmp_path\n",
        "    print(f\"File uploaded to {uploaded_file_path}\")\n",
        "\n",
        "    # Predict emotion and convert to text\n",
        "    dataset_emotions = [\"angry\", \"calm\", \"disgust\", \"fearful\", \"happy\", \"sad\", \"surprised\", \"neutral\"]\n",
        "    predicted_emotion_label = predict_emotion(uploaded_file_path, scaler)\n",
        "    predicted_emotion_text = emotion_to_text(predicted_emotion_label, dataset_emotions)\n",
        "\n",
        "    # Display the result\n",
        "    print(f\"Predicted Emotion: {predicted_emotion_text}\")\n",
        "\n",
        "# Function to predict emotion from user input\n",
        "def predict_emotion(file_path, scaler):\n",
        "    # Extract features from user input\n",
        "    user_input_features = extract_user_input_features(file_path)\n",
        "\n",
        "    # Reshape and normalize the input data\n",
        "    user_input_features = user_input_features.reshape(1, user_input_features.shape[0], 1)\n",
        "    user_input_features = scaler.transform(user_input_features.reshape(-1, user_input_features.shape[-1])).reshape(user_input_features.shape)\n",
        "\n",
        "    # Make a prediction\n",
        "    prediction = model.predict(user_input_features)\n",
        "\n",
        "    # Get the emotion label\n",
        "    emotion_label = np.argmax(prediction)\n",
        "\n",
        "    return emotion_label\n",
        "\n",
        "# Function to convert emotion label to text\n",
        "def emotion_to_text(emotion_label, emotions):\n",
        "    try:\n",
        "        emotion_text = emotions[emotion_label]\n",
        "    except IndexError:\n",
        "        emotion_text = \"Unknown Emotion\"\n",
        "    return emotion_text\n",
        "\n",
        "# Create a button widget\n",
        "button = widgets.Button(description=\"Upload and Classify File\")\n",
        "\n",
        "# Attach the function to the button's click event\n",
        "button.on_click(on_button_click)\n",
        "\n",
        "# Display the button\n",
        "display(button)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157,
          "referenced_widgets": [
            "b7724d8773104c01a9863bda88d89794",
            "33059199d822456b9dca2e6b2a2a869f",
            "86d417f04e9b4285bd1e1d6c81fc3431"
          ]
        },
        "id": "rWoDB1opmVrV",
        "outputId": "50895d6d-962d-493f-d5c7-a1fddbc8d067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Upload and Classify File', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7724d8773104c01a9863bda88d89794"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b3759b08-39b2-470f-8431-0901ced3ba11\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b3759b08-39b2-470f-8431-0901ced3ba11\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 03-01-01-01-02-02-03.wav to 03-01-01-01-02-02-03.wav\n",
            "File uploaded to /tmp/03-01-01-01-02-02-03.wav\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "Predicted Emotion: calm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the Model to Drive"
      ],
      "metadata": {
        "id": "vemvraSOENyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model to Google Drive\n",
        "model.save(\"/content/drive/MyDrive/Audio_Recognition/best_model.h5\")\n",
        "\n",
        "# Save the scaler to Google Drive\n",
        "scaler_filename = \"/content/drive/MyDrive/Audio_Recognition/scaler.save\"\n",
        "joblib.dump(scaler, scaler_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_wdrsjhoxBb",
        "outputId": "24346065-ddd8-4488-cf7d-9b4a44c62653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Audio_Recognition/scaler.save']"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1jZC04HurEfW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}